{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ddb32037",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import random\n",
    "import nagisa\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import TextVectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8b4b8d",
   "metadata": {},
   "source": [
    "kaggle dataset link: https://www.kaggle.com/datasets/team-ai/japaneseenglish-bilingual-corpus?resource=download  \n",
    "This particular dataset is translated by a human and even checked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dacac8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to extract the zip file\n",
    "data_folder_name=r'D:\\chronicles\\eng-jap-data'\n",
    "target_dir=r'D:\\chronicles\\archive.zip'\n",
    "shutil.unpack_archive(target_dir,data_folder_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73957fd3",
   "metadata": {},
   "source": [
    "This praticular dataset is stored in the form of XML.  \n",
    "Where <j> tag has the japanese sentence and <e> has the english sentence.  \n",
    "I have only considerd the english sentences which is checked rather than sentences which is just translated to english."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53e912b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.remove('D:\\chronicles\\eng-jap-data\\wiki_corpus_2.01\\GNM\\GNM00155.xml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc14517",
   "metadata": {},
   "source": [
    "In the below cell i am extracting the value of <j> and <e> tags and storing it in CSV file using pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7aaed327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In D:\\chronicles\\eng-jap-data\\wiki_corpus_2.01\\BDS folder\n",
      "In D:\\chronicles\\eng-jap-data\\wiki_corpus_2.01\\BLD folder\n",
      "In D:\\chronicles\\eng-jap-data\\wiki_corpus_2.01\\CLT folder\n",
      "In D:\\chronicles\\eng-jap-data\\wiki_corpus_2.01\\EPR folder\n",
      "In D:\\chronicles\\eng-jap-data\\wiki_corpus_2.01\\FML folder\n",
      "In D:\\chronicles\\eng-jap-data\\wiki_corpus_2.01\\GNM folder\n",
      "In D:\\chronicles\\eng-jap-data\\wiki_corpus_2.01\\HST folder\n",
      "In D:\\chronicles\\eng-jap-data\\wiki_corpus_2.01\\kyoto_lexicon.csv folder\n",
      "In D:\\chronicles\\eng-jap-data\\wiki_corpus_2.01\\LTT folder\n",
      "In D:\\chronicles\\eng-jap-data\\wiki_corpus_2.01\\PNM folder\n",
      "In D:\\chronicles\\eng-jap-data\\wiki_corpus_2.01\\readme.pdf folder\n",
      "In D:\\chronicles\\eng-jap-data\\wiki_corpus_2.01\\RLW folder\n",
      "In D:\\chronicles\\eng-jap-data\\wiki_corpus_2.01\\ROD folder\n",
      "In D:\\chronicles\\eng-jap-data\\wiki_corpus_2.01\\SAT folder\n",
      "In D:\\chronicles\\eng-jap-data\\wiki_corpus_2.01\\SCL folder\n",
      "In D:\\chronicles\\eng-jap-data\\wiki_corpus_2.01\\SNT folder\n",
      "In D:\\chronicles\\eng-jap-data\\wiki_corpus_2.01\\TTL folder\n",
      "In D:\\chronicles\\eng-jap-data\\wiki_corpus_2.01\\Wiki_Corpus_List_2.01.csv folder\n",
      "Extracting done!\n"
     ]
    }
   ],
   "source": [
    "directory='D:\\chronicles\\eng-jap-data\\wiki_corpus_2.01'\n",
    "for item in os.listdir(directory):\n",
    "    item=os.path.join(directory,item)\n",
    "    print(\"In {} folder\".format(item))\n",
    "    if os.path.isdir(item):\n",
    "        for xml_file in os.listdir(item):\n",
    "            xml_file_path=os.path.join(item,xml_file)\n",
    "            tree=ET.parse(xml_file_path)\n",
    "            root=tree.getroot()\n",
    "            for sen in root.findall(\"tit\"):\n",
    "                jap_sentence=sen.find(\"./j\").text\n",
    "                eng_sentence=sen.find(\"./e/[@type='check']\").text\n",
    "                with open('eng-to-jap','a',encoding=\"utf-8\") as f:\n",
    "                    f.write(str(eng_sentence))\n",
    "                    f.write(\"\\t\")\n",
    "                    f.write(jap_sentence)\n",
    "                    f.write(\"\\n\")\n",
    "print(\"Extracting done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a74b9003",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('eng-to-jap',encoding=\"utf-8\") as f:\n",
    "    lines=f.read().split(\"\\n\")[:-1]\n",
    "    text_pairs=[]\n",
    "    for line in lines:\n",
    "        eng, jap = line.split(\"\\t\")\n",
    "        jap= \"[start] \"+jap+\" [end]\"\n",
    "        text_pairs.append((eng,jap))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b24bcd9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Zoroku HAMAMURA, the first', '[start] 浜村蔵六 (初世) [end]')\n",
      "('Kyoto Prefectural Insho-Domoto Museum of Fine Arts', '[start] 京都府立堂本印象美術館 [end]')\n",
      "('MINAMOTO no Noriyori', '[start] 源範頼 [end]')\n"
     ]
    }
   ],
   "source": [
    "for _ in range(3):\n",
    "    print(random.choice(text_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00d775b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28220 total pairs\n",
      "19754 training pairs\n",
      "4233 validation pairs\n",
      "4233 test pairs\n"
     ]
    }
   ],
   "source": [
    "random.shuffle(text_pairs)\n",
    "num_val_samples=int(0.15*len(text_pairs))\n",
    "num_train_samples=len(text_pairs)-2*num_val_samples\n",
    "train_pairs=text_pairs[:num_train_samples]\n",
    "val_pairs=text_pairs[num_train_samples:num_train_samples+num_val_samples]\n",
    "test_pairs=text_pairs[num_train_samples+num_val_samples:]\n",
    "\n",
    "\n",
    "print(f\"{len(text_pairs)} total pairs\")\n",
    "print(f\"{len(train_pairs)} training pairs\")\n",
    "print(f\"{len(val_pairs)} validation pairs\")\n",
    "print(f\"{len(test_pairs)} test pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7bf5da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size=15000\n",
    "sequence_length=10\n",
    "batch_size=64\n",
    "\n",
    "def custom_standardization(input_string):\n",
    "    return nagisa.filter(input_string)\n",
    "\n",
    "eng_vectorization=TextVectorization(\n",
    "                  max_tokens=vocab_size,\n",
    "                  output_mode=\"int\",\n",
    "                  output_sequence_length=sequence_length)\n",
    "jap_vectorization=TextVectorization(\n",
    "                  max_tokens=vocab_size,\n",
    "                  output_mode=\"int\",\n",
    "                  output_sequence_length=sequence_length+1,\n",
    "#                   standardize=custom_standardization\n",
    "                )\n",
    "\n",
    "train_eng_texts=[pair[0] for pair in train_pairs]\n",
    "train_jap_texts=[pair[1] for pair in train_pairs]\n",
    "\n",
    "eng_vectorization.adapt(train_eng_texts)\n",
    "jap_vectorization.adapt(train_jap_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bc43feb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_datasets(eng,jap):\n",
    "    eng=eng_vectorization(eng)\n",
    "    jap=jap_vectorization(jap)\n",
    "    return ({\"encoder_inputes\":eng,\"decoder_inputs\":jap[:,:-1],},jap[:,1:])\n",
    "\n",
    "def make_datasets(pairs):\n",
    "    eng_texts, jap_texts = zip(*pairs)\n",
    "    eng_texts= list(eng_texts)\n",
    "    jap_texts=list(jap_texts)\n",
    "    dataset=tf.data.Dataset.from_tensor_slices((eng_texts,jap_texts))\n",
    "    dataset=dataset.batch(batch_size)\n",
    "    dataset=dataset.map(format_datasets)\n",
    "    return dataset.shuffle(2048).prefetch(16).cache()\n",
    "\n",
    "train_dataset=make_datasets(train_pairs)\n",
    "val_dataset=make_datasets(val_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4adc32c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chro",
   "language": "python",
   "name": "chro"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
